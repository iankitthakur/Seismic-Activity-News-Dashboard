{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iankitthakur/Seismic-Activity-News-Dashboard/blob/main/data_weave_Kiro3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. Create the Streamlit Application File (app.py)\n",
        "# ==============================================================================\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "import plotly.express as px\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from google.genai.errors import APIError\n",
        "from google import genai\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Initialize the client again within the Streamlit process\n",
        "try:\n",
        "    # Use the key from the environment variable set in the main Colab cell\n",
        "    client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "except Exception:\n",
        "    client = None\n",
        "\n",
        "# --- Data Fetching Functions (Unchanged) ---\n",
        "@st.cache_data(ttl=60*60*4) # Cache for 4 hours\n",
        "def fetch_earthquake_data(days=30, min_magnitude=3.0):\n",
        "    \"\"\"Fetches global earthquake data from USGS for the last 'days'.\"\"\"\n",
        "    st.info(f\"Fetching USGS Earthquake data (M â‰¥ {min_magnitude}) for the last {days} days...\")\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    start_time = end_time - timedelta(days=days)\n",
        "\n",
        "    USGS_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
        "\n",
        "    params = {\n",
        "        \"format\": \"geojson\",\n",
        "        \"starttime\": start_time.isoformat(),\n",
        "        \"endtime\": end_time.isoformat(),\n",
        "        \"minmagnitude\": min_magnitude,\n",
        "        \"orderby\": \"time-asc\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(USGS_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        features = data.get(\"features\", [])\n",
        "        earthquakes = []\n",
        "        for feature in features:\n",
        "            props = feature['properties']\n",
        "            dt_object = datetime.fromtimestamp(props['time'] / 1000).date()\n",
        "            earthquakes.append({\n",
        "                'Date': dt_object,\n",
        "                'Magnitude': props['mag']\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(earthquakes)\n",
        "        if df.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Aggregate by Day for Count\n",
        "        df_quakes_daily = df.groupby('Date').agg(\n",
        "            Earthquake_Count=('Magnitude', 'size'),\n",
        "            Max_Magnitude=('Magnitude', 'max')\n",
        "        ).reset_index()\n",
        "\n",
        "        return df_quakes_daily\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        st.error(f\"Error fetching earthquake data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "@st.cache_data(ttl=60*60*4)\n",
        "def get_daily_news_sentiment(days=30):\n",
        "    \"\"\"Generates synthetic sentiment scores using the Gemini API.\"\"\"\n",
        "    if not client:\n",
        "        st.error(\"Gemini Client not initialized. Check your GEMINI_API_KEY.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    st.info(f\"Generating synthetic daily news sentiment for the last {days} days using Gemini...\")\n",
        "\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    start_date = (datetime.now() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an expert financial and geopolitical analyst. Your task is to generate a statistically plausible time-series of average global news sentiment (compound VADER score) for a dashboard. The sentiment should be represented as a number between -1.0 (extremely negative) and 1.0 (extremely positive), with 0.0 being neutral.\n",
        "\n",
        "    Generate a daily sentiment score for the period from {start_date} to {end_date}. The daily scores should simulate fluctuations based on real-world events. For instance, most days should be close to 0.0 to 0.1 (slightly positive), but include some significant dips (e.g., -0.3 to -0.6) and occasional spikes (e.5 to 0.8) to simulate major news events. Ensure the time series is continuous.\n",
        "\n",
        "    Your response MUST be a single JSON array, where each object has two keys: 'Date' (in YYYY-MM-DD format) and 'Avg_Sentiment' (a float). DO NOT include any explanatory text, markdown formatting (like ```json), or notes outside of the JSON array.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash',\n",
        "            contents=\"Generate the requested time series data now.\",\n",
        "            config={\"system_instruction\": system_prompt, \"response_mime_type\": \"application/json\"}\n",
        "        )\n",
        "\n",
        "        # The response text should be pure JSON\n",
        "        sentiment_data = json.loads(response.text)\n",
        "        df_sentiment = pd.DataFrame(sentiment_data)\n",
        "        df_sentiment['Date'] = pd.to_datetime(df_sentiment['Date']).dt.date\n",
        "\n",
        "        return df_sentiment\n",
        "\n",
        "    except APIError as e:\n",
        "        st.error(f\"Gemini API Error: Check your API Key or rate limits. {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except json.JSONDecodeError:\n",
        "        st.error(\"Gemini API did not return valid JSON. Retrying may help.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# --- NEW FUNCTION FOR ROLLING AVERAGE ---\n",
        "def calculate_rolling_average(df, window=7):\n",
        "    \"\"\"Calculates a rolling average for earthquake count.\"\"\"\n",
        "    df['Rolling_Avg_Count'] = df['Earthquake_Count'].rolling(window=window, min_periods=1).mean()\n",
        "    return df\n",
        "# ----------------------------------------\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION FOR CANDLESTICK PLOT ---\n",
        "def plot_sentiment_volatility(df):\n",
        "    \"\"\"\n",
        "    Creates a candlestick-style plot to visualize the sentiment score and its simulated daily volatility.\n",
        "    \"\"\"\n",
        "    # Create synthetic daily range around the average sentiment (Fixed +/- 0.05)\n",
        "    df['Sentiment_High'] = df['Avg_Sentiment'] + 0.05\n",
        "    df['Sentiment_Low'] = df['Avg_Sentiment'] - 0.05\n",
        "    df['Sentiment_Open'] = df['Avg_Sentiment'] + 0.02\n",
        "    df['Sentiment_Close'] = df['Avg_Sentiment'] - 0.02\n",
        "\n",
        "    st.subheader(\"Generated News Sentiment Volatility (Candlestick View)\")\n",
        "    st.markdown(\"This chart uses a candlestick representation to visualize the daily sentiment score and a simulated fixed volatility range.\")\n",
        "\n",
        "    fig_candle = go.Figure(\n",
        "        data=[\n",
        "            go.Candlestick(\n",
        "                x=df['Date'],\n",
        "                open=df['Sentiment_Open'],\n",
        "                high=df['Sentiment_High'],\n",
        "                low=df['Sentiment_Low'],\n",
        "                close=df['Sentiment_Close'],\n",
        "                name='Simulated Sentiment Range',\n",
        "                increasing_line_color='green',\n",
        "                decreasing_line_color='red'\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    fig_candle.update_layout(\n",
        "        xaxis_rangeslider_visible=False,\n",
        "        yaxis_title=\"Sentiment Score (-1.0 to 1.0)\",\n",
        "        height=450\n",
        "    )\n",
        "    st.plotly_chart(fig_candle, use_container_width=True)\n",
        "# ----------------------------------------\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION FOR EARTHQUAKE HEATMAP ---\n",
        "def plot_earthquake_heatmap(df):\n",
        "    \"\"\"\n",
        "    Creates a calendar-style heatmap showing earthquake counts by day of week across weeks.\n",
        "    \"\"\"\n",
        "    st.subheader(\"Seismic Activity Heatmap (Weekly Count)\")\n",
        "    st.markdown(\"Visualizing the distribution of daily earthquake count across the entire selected time period.\")\n",
        "\n",
        "    # 1. Ensure 'Date' is datetime object\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # 2. Extract Weekday and Week Number\n",
        "    df['DayOfWeek'] = df['Date'].dt.day_name()\n",
        "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "\n",
        "    # Define the order of the days for proper visualization\n",
        "    day_order = ['Sunday', 'Saturday', 'Friday', 'Thursday', 'Wednesday', 'Tuesday', 'Monday']\n",
        "\n",
        "    # 3. Create the pivot table for the heatmap\n",
        "    heatmap_data = df.pivot_table(\n",
        "        values='Earthquake_Count',\n",
        "        index='DayOfWeek',\n",
        "        columns='WeekOfYear',\n",
        "        fill_value=0  # Fill days with no quakes as 0\n",
        "    )\n",
        "\n",
        "    # Reindex the rows to ensure days are in order (Sunday at top)\n",
        "    heatmap_data = heatmap_data.reindex(day_order, axis=0)\n",
        "\n",
        "    # 4. Create the Plotly Heatmap\n",
        "    fig_heatmap = px.imshow(\n",
        "        heatmap_data,\n",
        "        color_continuous_scale=\"Plasma\",\n",
        "        x=heatmap_data.columns,\n",
        "        y=heatmap_data.index,\n",
        "        labels=dict(x=\"Week Number\", y=\"Day of Week\", color=\"Quake Count\"),\n",
        "        aspect=\"auto\"\n",
        "    )\n",
        "\n",
        "    fig_heatmap.update_xaxes(side=\"top\")\n",
        "    fig_heatmap.update_layout(height=400)\n",
        "\n",
        "    st.plotly_chart(fig_heatmap, use_container_width=True)\n",
        "# ---------------------------------------------\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION TO ADD CREATOR INFO ---\n",
        "def add_creator_info(name):\n",
        "    \"\"\"Adds a small credit footer to the sidebar.\"\"\"\n",
        "    st.sidebar.markdown(\"---\")\n",
        "    st.sidebar.caption(f\"**Project Creator:** {name}\")\n",
        "    st.sidebar.caption(\"Data: USGS & Gemini API\")\n",
        "# ----------------------------------------\n",
        "\n",
        "\n",
        "# --- Main Dashboard Layout ---\n",
        "\n",
        "def main_dashboard():\n",
        "    st.set_page_config(layout=\"wide\", page_title=\"Earthquake vs. News Sentiment Mashup\")\n",
        "\n",
        "    st.title(\"ðŸŒŽ Earthquake Frequency vs. Global News Sentiment\")\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    st.sidebar.header(\"Configuration\")\n",
        "    days = st.sidebar.slider(\"Data Range (Days)\", min_value=7, max_value=90, value=30)\n",
        "    min_mag = st.sidebar.slider(\"Min Earthquake Magnitude\", min_value=2.5, max_value=5.0, value=3.0, step=0.5)\n",
        "\n",
        "    # CALL NEW CREATOR INFO FUNCTION\n",
        "    add_creator_info(\"Ankit Thakur\")\n",
        "\n",
        "    # Fetch data\n",
        "    df_quakes_daily = fetch_earthquake_data(days=days, min_magnitude=min_mag)\n",
        "    df_sentiment_daily = get_daily_news_sentiment(days=days)\n",
        "\n",
        "    if df_quakes_daily.empty or df_sentiment_daily.empty:\n",
        "        st.error(\"One or both data sources returned no data or an error occurred. Please check configuration/API keys and try again.\")\n",
        "        return\n",
        "\n",
        "    # Merge DataFrames on Date\n",
        "    df_merged = pd.merge(df_quakes_daily, df_sentiment_daily, on='Date', how='inner')\n",
        "    df_merged = df_merged.sort_values('Date')\n",
        "\n",
        "    # CALL NEW ROLLING AVERAGE FUNCTION\n",
        "    df_merged = calculate_rolling_average(df_merged)\n",
        "\n",
        "    st.header(\"Correlation Analysis\")\n",
        "    st.subheader(f\"Data Mapped: {df_merged['Date'].min().strftime('%b %d')} to {df_merged['Date'].max().strftime('%b %d')} (Total Days: {len(df_merged)})\")\n",
        "\n",
        "    # Calculate Correlation\n",
        "    corr = df_merged['Earthquake_Count'].corr(df_merged['Avg_Sentiment'])\n",
        "    st.markdown(f\"**Pearson Correlation (Quakes vs. Sentiment):** **`{corr:.4f}`** (A number close to 0 means no linear relationship.)\")\n",
        "\n",
        "    # --- Chart 1: Combined Time Series (Now includes Rolling Average) ---\n",
        "    st.subheader(\"Time Series: Quake Count, Rolling Average, and Sentiment\")\n",
        "\n",
        "    fig = px.line(\n",
        "        df_merged,\n",
        "        x='Date',\n",
        "        y='Earthquake_Count',\n",
        "        labels={'Earthquake_Count': f'Daily Earthquake Count (M â‰¥ {min_mag})'},\n",
        "        title=f'Daily Earthquake Count vs. Generated Global News Sentiment',\n",
        "        height=500\n",
        "    )\n",
        "\n",
        "    # Add Rolling Average Trace\n",
        "    fig.add_scatter(\n",
        "        x=df_merged['Date'],\n",
        "        y=df_merged['Rolling_Avg_Count'],\n",
        "        mode='lines',\n",
        "        name='7-Day Rolling Avg Quake Count',\n",
        "        line=dict(color='orange', dash='dot'),\n",
        "        yaxis='y1'\n",
        "    )\n",
        "\n",
        "    # Add Average Sentiment as a secondary y-axis\n",
        "    fig.add_scatter(\n",
        "        x=df_merged['Date'],\n",
        "        y=df_merged['Avg_Sentiment'],\n",
        "        mode='lines',\n",
        "        name='Gemini Generated Avg Sentiment',\n",
        "        yaxis='y2'\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        yaxis=dict(title=f'Earthquake Count (M â‰¥ {min_mag})', showgrid=False),\n",
        "        yaxis2=dict(\n",
        "            title='Avg Global News Sentiment (-1.0 to 1.0)',\n",
        "            overlaying='y',\n",
        "            side='right',\n",
        "            range=[-1, 1],\n",
        "            showgrid=True\n",
        "        ),\n",
        "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
        "        hovermode=\"x unified\"\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    # CALL SENTIMENT PLOT FUNCTION\n",
        "    plot_sentiment_volatility(df_merged)\n",
        "\n",
        "    # CALL HEATMAP FUNCTION\n",
        "    plot_earthquake_heatmap(df_merged)\n",
        "\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Raw Data Preview\")\n",
        "    st.dataframe(df_merged[['Date', 'Earthquake_Count', 'Rolling_Avg_Count', 'Max_Magnitude', 'Avg_Sentiment']].tail(10))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_dashboard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCtRwj2mZtw0",
        "outputId": "3d64b0f8-27d3-46fd-bd65-ddd04ace6a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "5v1YPoCMJWEr",
        "outputId": "32afbb31-a803-485c-ce5e-9a348c080435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜.kiroâ€™: File exists\n",
            "--- Starting ngrok and Streamlit ---\n",
            "\n",
            "\n",
            "ðŸŽ‰ Your Streamlit Dashboard is Live! Access it here:\n",
            "\n",
            "ðŸ‘‰ PUBLIC URL: NgrokTunnel: \"https://nonrepeated-colorational-kayson.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Dashboard running. To stop, interrupt this cell (â–  button).\n",
            "\n",
            "--- Streamlit and ngrok stopped. ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2686116259.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dashboard running. To stop, interrupt this cell (â–  button).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[ERROR] ngrok failed to connect. Did you set the correct NGROK_AUTH_TOKEN? Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 2. Setup and Installation\n",
        "# ==============================================================================\n",
        "!pip install streamlit pyngrok pandas requests plotly google-genai -q\n",
        "import os\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datetime import datetime, timedelta\n",
        "from pyngrok import ngrok\n",
        "from google import genai\n",
        "from google.genai.errors import APIError\n",
        "import plotly.graph_objects as go # Must keep this import here for the execution environment\n",
        "\n",
        "# Create the mandatory /.kiro directory for submission\n",
        "!mkdir .kiro\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Configuration (User Input)\n",
        "# ==============================================================================\n",
        "# --- REQUIRED: Replace these placeholders ---\n",
        "NGROK_AUTH_TOKEN = \"36hQryOC6fkZfCmSfMqropVyvRy_2x9ZdYRbG9AQQKzDHbWwY\"\n",
        "# Get your Gemini API Key from Google AI Studio (ai.google.dev)\n",
        "# For secure storage in Colab, use the secrets manager (ðŸ”‘ icon on the left panel).\n",
        "# Name your secret 'GEMINI_API_KEY' if you wish to use the code below.\n",
        "from google.colab import userdata\n",
        "\n",
        "if 'GEMINI_API_KEY' not in os.environ:\n",
        "    try:\n",
        "        GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"Warning: GEMINI_API_KEY not found in Colab secrets. Please add it or set it manually.\")\n",
        "        GEMINI_API_KEY = \"AIzaSyAoKKO7RcPZB7jQYoGuX5H3t0v5WVgwByk\" # Fallback for execution\n",
        "# ---------------------------------------------\n",
        "\n",
        "# Set the environment variable for the Gemini client\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "# Initialize the Gemini Client\n",
        "try:\n",
        "    gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "except ValueError:\n",
        "    print(\"Warning: GEMINI_API_KEY is not set correctly. The sentiment analysis will fail.\")\n",
        "    gemini_client = None\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. ngrok Setup and Execution\n",
        "# ==============================================================================\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "print(\"--- Starting ngrok and Streamlit ---\")\n",
        "\n",
        "# 1. Authenticate ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# 2. Run Streamlit in the background\n",
        "def run_streamlit():\n",
        "    subprocess.Popen([\n",
        "        \"streamlit\",\n",
        "        \"run\",\n",
        "        \"app.py\",\n",
        "        \"--server.port\", \"8501\",\n",
        "        \"--server.headless\", \"true\"\n",
        "    ])\n",
        "\n",
        "# Start Streamlit in a separate thread\n",
        "threading.Thread(target=run_streamlit, daemon=True).start()\n",
        "time.sleep(15) # Give Streamlit time to start\n",
        "\n",
        "# 3. Create ngrok tunnel\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"\\n\\nðŸŽ‰ Your Streamlit Dashboard is Live! Access it here:\\n\")\n",
        "    print(f\"ðŸ‘‰ PUBLIC URL: {public_url}\\n\")\n",
        "    print(\"-----------------------------------------------------------\\n\")\n",
        "\n",
        "    # Keep the Colab cell running until interrupted\n",
        "    print(\"Dashboard running. To stop, interrupt this cell (â–  button).\")\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] ngrok failed to connect. Did you set the correct NGROK_AUTH_TOKEN? Error: {e}\")\n",
        "finally:\n",
        "    # Cleanup on stop\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "    except:\n",
        "        pass\n",
        "    print(\"\\n--- Streamlit and ngrok stopped. ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/wKig5B7UTR4IE52lkZeO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}